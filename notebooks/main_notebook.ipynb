{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# News Article Semantic Similarity & Topic Retrieval Using Contrastive Learning\n",
        "\n",
        "This notebook implements a complete contrastive learning pipeline for news article semantic similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "Make sure you have installed all required packages:\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from src.data_loader import preprocess_dataset\n",
        "\n",
        "# Load AG News dataset\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# Inspect sample\n",
        "print(\"Sample from train set:\")\n",
        "print(dataset['train'][0])\n",
        "print(f\"\\nTrain size: {len(dataset['train'])}, Test size: {len(dataset['test'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocess Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess dataset\n",
        "dataset = preprocess_dataset(dataset)\n",
        "\n",
        "# Check preprocessed sample\n",
        "print(\"Preprocessed sample:\")\n",
        "print(dataset['train'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build Anchor-Positive-Negative Triplets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.triplets import create_triplets_from_dataset\n",
        "\n",
        "# Create triplets (limit for demo)\n",
        "triplets = create_triplets_from_dataset(dataset['train'], max_triplets=2000)\n",
        "\n",
        "print(f\"Created {len(triplets)} triplets\")\n",
        "print(\"\\nSample triplet:\")\n",
        "print(f\"Anchor: {triplets[0][0][:100]}...\")\n",
        "print(f\"Positive: {triplets[0][1][:100]}...\")\n",
        "print(f\"Negative: {triplets[0][2][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load Pre-Trained Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"Model loaded: {model.get_sentence_embedding_dimension()} dimensions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Baseline Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.baseline import BaselineEvaluator\n",
        "from src.data_loader import get_text_and_labels\n",
        "\n",
        "# Get test samples\n",
        "texts, labels = get_text_and_labels(dataset['test'], max_samples=500)\n",
        "\n",
        "# Evaluate baseline\n",
        "baseline_evaluator = BaselineEvaluator(model_name='all-MiniLM-L6-v2')\n",
        "baseline_embeddings = baseline_evaluator.encode(texts)\n",
        "\n",
        "# Test similarity search\n",
        "results = baseline_evaluator.compute_similarity(0, top_k=5)\n",
        "print(\"\\nTop 5 similar articles to query:\")\n",
        "for idx, score in results:\n",
        "    print(f\"  [{idx}] (score: {score:.4f}): {texts[idx][:80]}...\")\n",
        "\n",
        "# Visualize\n",
        "baseline_evaluator.visualize_embeddings(labels, save_path=\"../baseline_embeddings.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Prepare DataLoader for Contrastive Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import InputExample, losses\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Prepare training examples\n",
        "train_examples = [InputExample(texts=[a, p, n]) for a, p, n in triplets[:2000]]\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
        "\n",
        "print(f\"Prepared {len(train_examples)} training examples\")\n",
        "print(f\"Batch size: {train_dataloader.batch_size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Model with Contrastive Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.training import ContrastiveTrainer\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = ContrastiveTrainer(base_model_name='all-MiniLM-L6-v2')\n",
        "\n",
        "# Prepare dataloader\n",
        "train_dataloader = trainer.prepare_dataloader(triplets[:2000], batch_size=32)\n",
        "\n",
        "# Train with triplet loss\n",
        "num_epochs = 2\n",
        "trainer.train(\n",
        "    train_dataloader,\n",
        "    loss_type='triplet',\n",
        "    num_epochs=num_epochs,\n",
        "    output_path='../models/news_contrastive_model',\n",
        "    save_model=True\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Evaluate Fine-Tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.evaluation import Evaluator\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load fine-tuned model\n",
        "finetuned_model = SentenceTransformer('../models/news_contrastive_model')\n",
        "\n",
        "# Evaluate\n",
        "evaluator = Evaluator(finetuned_model, texts, labels)\n",
        "results = evaluator.evaluate_all(k_values=[1, 5, 10])\n",
        "\n",
        "# Visualize improved embeddings\n",
        "evaluator.visualize_embeddings(\n",
        "    labels=labels,\n",
        "    save_path=\"../finetuned_embeddings.png\",\n",
        "    title=\"Fine-tuned Embeddings\"\n",
        ")\n",
        "\n",
        "# Compare with baseline\n",
        "comparison = evaluator.compare_with_baseline(baseline_embeddings, k_values=[1, 5, 10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Hard Negative Mining (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.hard_negatives import HardNegativeMiner\n",
        "from src.data_loader import get_text_and_labels\n",
        "\n",
        "# Get corpus for hard negative mining\n",
        "corpus_texts, _ = get_text_and_labels(dataset['train'], max_samples=1000)\n",
        "\n",
        "# Build BM25 index\n",
        "miner = HardNegativeMiner()\n",
        "miner.build_bm25_index(corpus_texts)\n",
        "\n",
        "# Mine hard negatives\n",
        "query = \"breaking news in politics\"\n",
        "hard_negatives = miner.mine_bm25_hard_negatives(query, n=5)\n",
        "\n",
        "print(f\"Hard negatives for '{query}':\")\n",
        "for idx in hard_negatives:\n",
        "    print(f\"  [{idx}]: {corpus_texts[idx][:80]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Math Behind InfoNCE Loss\n",
        "\n",
        "The InfoNCE loss is defined as:\n",
        "\n",
        "$$\\\\mathcal{L}_{i} = - \\\\log \\\\frac{\\\\exp(\\\\text{sim}(x_i, x_i^+)/\\\\tau)}{\\\\sum_{j=0}^{N} \\\\exp(\\\\text{sim}(x_i, x_j)/\\\\tau)}$$\n",
        "\n",
        "Where:\n",
        "- $x_i$ is the anchor\n",
        "- $x_i^+$ is the positive sample\n",
        "- $\\\\tau$ is the temperature hyperparameter\n",
        "- $\\\\text{sim}$ is the cosine similarity function\n",
        "\n",
        "See `docs/loss_explanation.md` for detailed explanation.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
